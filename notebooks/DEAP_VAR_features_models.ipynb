{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46615aba-5a8d-4cc3-942c-122503dc24e5",
   "metadata": {},
   "source": [
    "# Построим модель из признаков модели VAR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3f3047-580b-422e-afd3-a09f1db4de50",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Для начала разобьем DEAP на трейн и тест:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfcce13e-6803-494c-b03f-ff8743bc43b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import random\n",
    "from typing import List\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abb606bf-235b-4405-acee-f07ff9d6efd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# путь к DEAP\n",
    "prepocessed_DEAP_path = '' # путь к предобработанному DEAP\n",
    "deap_lstdir = os.listdir(prepocessed_DEAP_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b47da54f-0c6f-4f4e-8da7-dc87306dab3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# создаем общий датасет, который после будем разделять\n",
    "\n",
    "# проходим по всем испытуемым\n",
    "subject_dicts = []\n",
    "for subject_path in deap_lstdir:\n",
    "    path = os.path.join(prepocessed_DEAP_path, subject_path)\n",
    "    with open(path, 'rb') as file:\n",
    "        subject = pickle.load(file, encoding='latin1')\n",
    "        subject_dicts.append(subject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12d0cda5-f3f0-4ab8-80fa-eaaf9bd19727",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quadrant_fill(val_arous_tuple: tuple):\n",
    "    valence, arousal = val_arous_tuple\n",
    "    if valence == 'high' and arousal == 'high':\n",
    "        return 'HAHV'\n",
    "    elif valence == 'low' and arousal == 'high':\n",
    "        return 'HALV'\n",
    "    elif valence == 'low' and arousal == 'low':\n",
    "        return 'LALV'\n",
    "    elif valence == 'high' and arousal == 'low':\n",
    "        return 'LAHV'\n",
    "\n",
    "def build_deap_dataframe(subject_dicts) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    subject_dicts: список словарей (по одному на каждого испытуемого),\n",
    "                   каждый словарь должен иметь:\n",
    "                       'data'   -> shape (40, 40, 8064)\n",
    "                       'labels' -> shape (40, 4)  (valence, arousal, dominance, liking)\n",
    "\n",
    "    Возвращает: pd.DataFrame со столбцами:\n",
    "      [subject_id, video_id, data, valence, arousal, dominance, liking]\n",
    "    \"\"\"\n",
    "\n",
    "    all_rows = []\n",
    "\n",
    "    for subject_id, subject_dict in enumerate(subject_dicts, start=1):\n",
    "        data_3d = subject_dict['data']    # shape = (40, 40, 8064)\n",
    "        labels_2d = subject_dict['labels']  # shape = (40, 4)\n",
    "\n",
    "        n_trials, n_channels, n_points = data_3d.shape\n",
    "        n_channels = 32 # берем только ЭЭГ каналы\n",
    "        fs = 128\n",
    "        seg_length = 10 * fs  # 10 секунд * 128 Гц = 1280\n",
    "\n",
    "        for trial_id in range(n_trials):\n",
    "            # Извлекаем (40, 8064)\n",
    "            trial_signal = data_3d[trial_id, :n_channels, :]\n",
    "            \n",
    "            # Извлекаем метки (valence, arousal, dominance, liking) для данного триала\n",
    "            valence, arousal, dominance, liking = labels_2d[trial_id]\n",
    "\n",
    "            # Формируем строку будущего DataFrame\n",
    "            row_dict = {\n",
    "                'subject_id': subject_id,\n",
    "                'video_id': trial_id + 1,\n",
    "                'data': trial_signal,  \n",
    "                'valence': valence,\n",
    "                'arousal': arousal,\n",
    "                'dominance': dominance,\n",
    "                'liking': liking\n",
    "            }\n",
    "            all_rows.append(row_dict)\n",
    "\n",
    "    # Создаём DataFrame \n",
    "    df = pd.DataFrame(all_rows)\n",
    "    # сделаем задачу классификации\n",
    "    df['Low/high valence'] = df['valence'].apply(lambda x: 'low' if x <= 5 else 'high')\n",
    "    df['Low/high arousal'] = df['arousal'].apply(lambda x: 'low' if x <= 5 else 'high')\n",
    "    df['quadrant'] = df.apply(lambda x: quadrant_fill((x['Low/high valence'], x['Low/high arousal'])), axis=1)\n",
    "    return df\n",
    "\n",
    "def deap_train_test_split(deap_df: pd.DataFrame) -> List[pd.DataFrame]:\n",
    "    random.seed(42)\n",
    "    subject_idx = random.sample(range(1, 33),6)\n",
    "    video_idx = random.sample(range(1, 41),8)\n",
    "\n",
    "    train_df =  deap_df[(~deap_df['video_id'].isin(video_idx)) & (~deap_df['subject_id'].isin(subject_idx))]\n",
    "    test_df = deap_df[(deap_df['video_id'].isin(video_idx)) & (deap_df['subject_id'].isin(subject_idx))]\n",
    "    \n",
    "    test_new_subj_old_vid = deap_df[(~deap_df['video_id'].isin(video_idx) & (deap_df['subject_id'].isin(subject_idx)))]\n",
    "    test_new_vid_old_subj = deap_df[(~deap_df['subject_id'].isin(subject_idx) & (deap_df['video_id'].isin(video_idx)))]\n",
    "\n",
    "    return train_df, test_df, test_new_subj_old_vid, test_new_vid_old_subj # len: 832, 48, 1024, 1040\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b48b92d-d0e8-449d-afb3-0892686cf707",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>video_id</th>\n",
       "      <th>data</th>\n",
       "      <th>valence</th>\n",
       "      <th>arousal</th>\n",
       "      <th>dominance</th>\n",
       "      <th>liking</th>\n",
       "      <th>Low/high valence</th>\n",
       "      <th>Low/high arousal</th>\n",
       "      <th>quadrant</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[[0.948231680995192, 1.65333532651348, 3.01372...</td>\n",
       "      <td>7.71</td>\n",
       "      <td>7.60</td>\n",
       "      <td>6.90</td>\n",
       "      <td>7.83</td>\n",
       "      <td>high</td>\n",
       "      <td>high</td>\n",
       "      <td>HAHV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>[[10.260175049914748, 12.795442725569648, 10.4...</td>\n",
       "      <td>8.10</td>\n",
       "      <td>7.31</td>\n",
       "      <td>7.28</td>\n",
       "      <td>8.47</td>\n",
       "      <td>high</td>\n",
       "      <td>high</td>\n",
       "      <td>HAHV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>[[1.0130495576625123, -1.0678322951836536, 3.9...</td>\n",
       "      <td>8.58</td>\n",
       "      <td>7.54</td>\n",
       "      <td>9.00</td>\n",
       "      <td>7.08</td>\n",
       "      <td>high</td>\n",
       "      <td>high</td>\n",
       "      <td>HAHV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>[[-7.658428424515396, -3.2675578443273143, 0.7...</td>\n",
       "      <td>4.94</td>\n",
       "      <td>6.01</td>\n",
       "      <td>6.12</td>\n",
       "      <td>8.06</td>\n",
       "      <td>low</td>\n",
       "      <td>high</td>\n",
       "      <td>HALV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>[[-1.8111079228929805, -4.7838764286765, -0.52...</td>\n",
       "      <td>6.96</td>\n",
       "      <td>3.92</td>\n",
       "      <td>7.19</td>\n",
       "      <td>6.05</td>\n",
       "      <td>high</td>\n",
       "      <td>low</td>\n",
       "      <td>LAHV</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subject_id  video_id                                               data  \\\n",
       "0           1         1  [[0.948231680995192, 1.65333532651348, 3.01372...   \n",
       "1           1         2  [[10.260175049914748, 12.795442725569648, 10.4...   \n",
       "2           1         3  [[1.0130495576625123, -1.0678322951836536, 3.9...   \n",
       "3           1         4  [[-7.658428424515396, -3.2675578443273143, 0.7...   \n",
       "4           1         5  [[-1.8111079228929805, -4.7838764286765, -0.52...   \n",
       "\n",
       "   valence  arousal  dominance  liking Low/high valence Low/high arousal  \\\n",
       "0     7.71     7.60       6.90    7.83             high             high   \n",
       "1     8.10     7.31       7.28    8.47             high             high   \n",
       "2     8.58     7.54       9.00    7.08             high             high   \n",
       "3     4.94     6.01       6.12    8.06              low             high   \n",
       "4     6.96     3.92       7.19    6.05             high              low   \n",
       "\n",
       "  quadrant  \n",
       "0     HAHV  \n",
       "1     HAHV  \n",
       "2     HAHV  \n",
       "3     HALV  \n",
       "4     LAHV  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = build_deap_dataframe(subject_dicts)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a8d1b4b6-d081-4066-b798-c7ec6f00040d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df, test_new_subj_old_vid, test_new_vid_old_subj = deap_train_test_split(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714b7988-60c5-4da7-8eac-9e51f24e35f5",
   "metadata": {},
   "source": [
    "### Обучим модель VAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5d2cb2f6-f7d5-4d35-8d20-059cc8b6ba34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.api import VAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2ef75db4-b71a-49de-9cc2-0fcd442e04d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_var_features(array, best_p=5, last_sec = 10, fs=128):\n",
    "    # Для VAR statsmodels нужно (time, channels) => (1280, 40)\n",
    "    seg_length = last_sec * fs\n",
    "    last_segment = array[:, -seg_length:]\n",
    "    \n",
    "    data_var = last_segment.T  # (1280, 40)\n",
    "\n",
    "    # Обучаем VAR на выбранном p\n",
    "    model = VAR(data_var)\n",
    "    results = model.fit(best_p)\n",
    "\n",
    "    # Извлекаем коэффициенты (shape = (p, k, k)) и интерсепт (shape = (k,))\n",
    "    coefs = results.coefs      # матрицы A_i\n",
    "    intercept = results.intercept  # вектор c\n",
    "    # Превратим всё в один вектор\n",
    "    # (p*k*k) + (k) признаков\n",
    "    var_features = np.concatenate([coefs.flatten(), intercept.flatten()])\n",
    "\n",
    "    return var_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "26d306cd-91f8-4c84-ab79-2fbbd6426464",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_24200\\103710021.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df['var_features'] = train_df['data'].apply(lambda x: get_var_features(x))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>video_id</th>\n",
       "      <th>data</th>\n",
       "      <th>valence</th>\n",
       "      <th>arousal</th>\n",
       "      <th>dominance</th>\n",
       "      <th>liking</th>\n",
       "      <th>Low/high valence</th>\n",
       "      <th>Low/high arousal</th>\n",
       "      <th>quadrant</th>\n",
       "      <th>var_features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>[[-11.11631040230457, 71.81244100609055, 122.8...</td>\n",
       "      <td>9.00</td>\n",
       "      <td>5.03</td>\n",
       "      <td>7.13</td>\n",
       "      <td>6.62</td>\n",
       "      <td>high</td>\n",
       "      <td>high</td>\n",
       "      <td>HAHV</td>\n",
       "      <td>[-26270538892.56006, -26270538892.99161, -2627...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>[[-171.79711783177316, -117.17857114425763, -4...</td>\n",
       "      <td>6.05</td>\n",
       "      <td>1.00</td>\n",
       "      <td>5.04</td>\n",
       "      <td>7.03</td>\n",
       "      <td>high</td>\n",
       "      <td>low</td>\n",
       "      <td>LAHV</td>\n",
       "      <td>[-24367199887.515038, -24367199889.032898, -24...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>[[1.4713609744647984, -38.936057294245664, -66...</td>\n",
       "      <td>5.04</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.65</td>\n",
       "      <td>5.04</td>\n",
       "      <td>high</td>\n",
       "      <td>low</td>\n",
       "      <td>LAHV</td>\n",
       "      <td>[10418081987.315346, 10418081986.42015, 104180...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>[[15.944453976224564, 11.812813111443923, 7.97...</td>\n",
       "      <td>9.00</td>\n",
       "      <td>9.00</td>\n",
       "      <td>9.00</td>\n",
       "      <td>9.00</td>\n",
       "      <td>high</td>\n",
       "      <td>high</td>\n",
       "      <td>HAHV</td>\n",
       "      <td>[1.7698641691775852, -0.1805263829047668, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>[[178.77559036713717, 144.71389583087222, 97.0...</td>\n",
       "      <td>4.99</td>\n",
       "      <td>1.00</td>\n",
       "      <td>9.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>LALV</td>\n",
       "      <td>[14366347983.066248, 14366347983.031427, 14366...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1233</th>\n",
       "      <td>31</td>\n",
       "      <td>34</td>\n",
       "      <td>[[20.488056516461008, 27.592511136336057, 25.5...</td>\n",
       "      <td>1.95</td>\n",
       "      <td>8.03</td>\n",
       "      <td>2.08</td>\n",
       "      <td>1.00</td>\n",
       "      <td>low</td>\n",
       "      <td>high</td>\n",
       "      <td>HALV</td>\n",
       "      <td>[54136014418.20242, 54136014417.449615, 541360...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1235</th>\n",
       "      <td>31</td>\n",
       "      <td>36</td>\n",
       "      <td>[[0.5274022253354644, -2.4252270719619826, 5.0...</td>\n",
       "      <td>4.97</td>\n",
       "      <td>6.95</td>\n",
       "      <td>1.96</td>\n",
       "      <td>1.96</td>\n",
       "      <td>low</td>\n",
       "      <td>high</td>\n",
       "      <td>HALV</td>\n",
       "      <td>[0.9553039723074183, 0.041257079745979613, 0.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1237</th>\n",
       "      <td>31</td>\n",
       "      <td>38</td>\n",
       "      <td>[[-10.197774986718422, -4.8967558966950655, 4....</td>\n",
       "      <td>1.00</td>\n",
       "      <td>9.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>low</td>\n",
       "      <td>high</td>\n",
       "      <td>HALV</td>\n",
       "      <td>[0.9520495133535651, 0.0743042161171885, 0.668...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1238</th>\n",
       "      <td>31</td>\n",
       "      <td>39</td>\n",
       "      <td>[[-14.439967892369696, -5.040654886024263, 4.0...</td>\n",
       "      <td>1.00</td>\n",
       "      <td>9.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.01</td>\n",
       "      <td>low</td>\n",
       "      <td>high</td>\n",
       "      <td>HALV</td>\n",
       "      <td>[16144239026.216217, 16144239025.354963, 16144...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1239</th>\n",
       "      <td>31</td>\n",
       "      <td>40</td>\n",
       "      <td>[[2.9737238833278203, -0.6965378232826276, -5....</td>\n",
       "      <td>3.99</td>\n",
       "      <td>8.06</td>\n",
       "      <td>2.09</td>\n",
       "      <td>2.03</td>\n",
       "      <td>low</td>\n",
       "      <td>high</td>\n",
       "      <td>HALV</td>\n",
       "      <td>[133598386854.29457, 133598386853.51775, 13359...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>832 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      subject_id  video_id                                               data  \\\n",
       "40             2         1  [[-11.11631040230457, 71.81244100609055, 122.8...   \n",
       "43             2         4  [[-171.79711783177316, -117.17857114425763, -4...   \n",
       "44             2         5  [[1.4713609744647984, -38.936057294245664, -66...   \n",
       "47             2         8  [[15.944453976224564, 11.812813111443923, 7.97...   \n",
       "49             2        10  [[178.77559036713717, 144.71389583087222, 97.0...   \n",
       "...          ...       ...                                                ...   \n",
       "1233          31        34  [[20.488056516461008, 27.592511136336057, 25.5...   \n",
       "1235          31        36  [[0.5274022253354644, -2.4252270719619826, 5.0...   \n",
       "1237          31        38  [[-10.197774986718422, -4.8967558966950655, 4....   \n",
       "1238          31        39  [[-14.439967892369696, -5.040654886024263, 4.0...   \n",
       "1239          31        40  [[2.9737238833278203, -0.6965378232826276, -5....   \n",
       "\n",
       "      valence  arousal  dominance  liking Low/high valence Low/high arousal  \\\n",
       "40       9.00     5.03       7.13    6.62             high             high   \n",
       "43       6.05     1.00       5.04    7.03             high              low   \n",
       "44       5.04     3.00       3.65    5.04             high              low   \n",
       "47       9.00     9.00       9.00    9.00             high             high   \n",
       "49       4.99     1.00       9.00    1.00              low              low   \n",
       "...       ...      ...        ...     ...              ...              ...   \n",
       "1233     1.95     8.03       2.08    1.00              low             high   \n",
       "1235     4.97     6.95       1.96    1.96              low             high   \n",
       "1237     1.00     9.00       1.00    1.00              low             high   \n",
       "1238     1.00     9.00       1.00    1.01              low             high   \n",
       "1239     3.99     8.06       2.09    2.03              low             high   \n",
       "\n",
       "     quadrant                                       var_features  \n",
       "40       HAHV  [-26270538892.56006, -26270538892.99161, -2627...  \n",
       "43       LAHV  [-24367199887.515038, -24367199889.032898, -24...  \n",
       "44       LAHV  [10418081987.315346, 10418081986.42015, 104180...  \n",
       "47       HAHV  [1.7698641691775852, -0.1805263829047668, -0.0...  \n",
       "49       LALV  [14366347983.066248, 14366347983.031427, 14366...  \n",
       "...       ...                                                ...  \n",
       "1233     HALV  [54136014418.20242, 54136014417.449615, 541360...  \n",
       "1235     HALV  [0.9553039723074183, 0.041257079745979613, 0.1...  \n",
       "1237     HALV  [0.9520495133535651, 0.0743042161171885, 0.668...  \n",
       "1238     HALV  [16144239026.216217, 16144239025.354963, 16144...  \n",
       "1239     HALV  [133598386854.29457, 133598386853.51775, 13359...  \n",
       "\n",
       "[832 rows x 11 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['var_features'] = train_df['data'].apply(lambda x: get_var_features(x))\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "222e1a21-c25e-474d-a036-81ebae67629b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_24200\\4245031833.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df['var_features'] = test_df['data'].apply(lambda x: get_var_features(x))\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_24200\\4245031833.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_new_subj_old_vid['var_features'] = test_new_subj_old_vid['data'].apply(lambda x: get_var_features(x))\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_24200\\4245031833.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_new_vid_old_subj['var_features'] = test_new_vid_old_subj['data'].apply(lambda x: get_var_features(x))\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "test_df['var_features'] = test_df['data'].apply(lambda x: get_var_features(x))\n",
    "test_new_subj_old_vid['var_features'] = test_new_subj_old_vid['data'].apply(lambda x: get_var_features(x))\n",
    "test_new_vid_old_subj['var_features'] = test_new_vid_old_subj['data'].apply(lambda x: get_var_features(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888856b7-770b-46e7-8a82-4efc486aa5f5",
   "metadata": {},
   "source": [
    "### Проведем бутстрап проверку моделей\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dcce5b16-8d80-4c63-ab24-71826d918645",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2cdfcfcc-2c2c-4177-b6ef-96b753644ca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_24200\\832171606.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['var_features'] = list(X_scaled)  # Записываем обратно в DataFrame\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_24200\\832171606.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['quadrant'] = y_digit\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_24200\\832171606.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['var_features'] = list(X_scaled)  # Записываем обратно в DataFrame\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_24200\\832171606.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['quadrant'] = y_digit\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_24200\\832171606.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['var_features'] = list(X_scaled)  # Записываем обратно в DataFrame\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_24200\\832171606.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['quadrant'] = y_digit\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_24200\\832171606.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['var_features'] = list(X_scaled)  # Записываем обратно в DataFrame\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_24200\\832171606.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['quadrant'] = y_digit\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "scaler = StandardScaler()\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Обучаем scaler на train (все данные объединены в один массив)\n",
    "X_train = np.stack(train_df['var_features'].values, axis=0)\n",
    "y_train = train_df['quadrant']\n",
    "scaler.fit(X_train)\n",
    "le.fit(y_train)\n",
    "\n",
    "# Функция для нормализации фичей в DataFrame\n",
    "def normalize_features(df, scaler, le):\n",
    "    X = np.stack(df['var_features'].values, axis=0)\n",
    "    X_scaled = scaler.transform(X)  # Применяем трансформацию\n",
    "    y_digit = le.transform(df['quadrant'])\n",
    "    df['var_features'] = list(X_scaled)  # Записываем обратно в DataFrame\n",
    "    df['quadrant'] = y_digit\n",
    "    return df\n",
    "\n",
    "# Применяем scaler к каждому DataFrame\n",
    "train_df_epochs = normalize_features(train_df, scaler, le)\n",
    "test_df_epochs = normalize_features(test_df, scaler, le)\n",
    "test_new_subj_old_vid_epochs = normalize_features(test_new_subj_old_vid, scaler, le)\n",
    "test_new_vid_old_subj_epochs = normalize_features(test_new_vid_old_subj, scaler, le)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d2ab9e63-94b0-46c2-a64f-167e2e2a0ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_metric(y_true, y_pred, metric_fn, samples_cnt=1000, random_state=42):\n",
    "    np.random.seed(random_state)\n",
    "    y_true = np.array(y_true)  # Ensure NumPy array\n",
    "    y_pred = np.array(y_pred)  # Ensure NumPy array\n",
    "    b_metric = np.zeros(samples_cnt)\n",
    "    for i in range(samples_cnt):\n",
    "        poses = np.random.choice(y_true.shape[0], size=y_true.shape[0], replace=True)\n",
    "\n",
    "        y_true_boot = y_true[poses]\n",
    "        y_pred_boot = y_pred[poses]\n",
    "        m_val = metric_fn(y_true_boot, y_pred_boot)\n",
    "        b_metric[i] = m_val\n",
    "\n",
    "    return b_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "19839de4-19fa-44b0-9206-d0f8813c4a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test_classifier(models: Dict, train_df_epochs, test_df_epochs,\n",
    "                              test_new_subj_old_vid_epochs, test_new_vid_old_subj_epochs, verb=True) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Функция берет models: dict, который содержит инициализированные модели, обучает их, \n",
    "    а после тестирует с помощью balanced_accuracy. Результат обучения заносится в pd.DataFrame.\n",
    "    \"\"\"\n",
    "    # train\n",
    "    x_train = np.stack(train_df_epochs['var_features'].values, axis=0)\n",
    "    y_train = train_df_epochs['quadrant']\n",
    "\n",
    "    # test\n",
    "    x_test = np.stack(test_df_epochs['var_features'].values, axis=0)\n",
    "    y_test = test_df_epochs['quadrant']\n",
    "\n",
    "    # corner test\n",
    "    x_test_new_subj_old_vid = np.stack(test_new_subj_old_vid_epochs['var_features'].values, axis=0)\n",
    "    y_test_new_subj_old_vid = test_new_subj_old_vid_epochs['quadrant']\n",
    "\n",
    "    x_test_new_vid_old_subj = np.stack(test_new_vid_old_subj_epochs['var_features'].values, axis=0)\n",
    "    y_test_new_vid_old_subj = test_new_vid_old_subj_epochs['quadrant']\n",
    "\n",
    "    results_list = []  # Список для хранения всех результатов\n",
    "\n",
    "    for name, model in models.items():\n",
    "        model.fit(x_train, y_train)\n",
    "\n",
    "        # Обычное тестирование\n",
    "        y_pred = model.predict(x_test)\n",
    "        test_scores = bootstrap_metric(y_test, y_pred, metric_fn=balanced_accuracy_score)\n",
    "        test_mean, test_ci_lower, test_ci_upper = np.mean(test_scores), np.percentile(test_scores, 2.5), np.percentile(test_scores, 97.5)\n",
    "\n",
    "        # \"Новые субъекты, старые видео\"\n",
    "        y_pred_new_subj_old_vid = model.predict(x_test_new_subj_old_vid)\n",
    "        test_new_subj_old_vid_scores = bootstrap_metric(y_test_new_subj_old_vid, y_pred_new_subj_old_vid, \n",
    "                                                         metric_fn=balanced_accuracy_score)\n",
    "        test_new_subj_old_vid_mean, test_new_subj_old_vid_ci_lower, test_new_subj_old_vid_ci_upper = (\n",
    "            np.mean(test_new_subj_old_vid_scores),\n",
    "            np.percentile(test_new_subj_old_vid_scores, 2.5),\n",
    "            np.percentile(test_new_subj_old_vid_scores, 97.5),\n",
    "        )\n",
    "\n",
    "        # \"Новые видео, старые субъекты\"\n",
    "        y_pred_new_vid_old_subj = model.predict(x_test_new_vid_old_subj)\n",
    "        test_new_vid_old_subj_scores = bootstrap_metric(y_test_new_vid_old_subj, y_pred_new_vid_old_subj, \n",
    "                                                         metric_fn=balanced_accuracy_score)\n",
    "        test_new_vid_old_subj_mean, test_new_vid_old_subj_ci_lower, test_new_vid_old_subj_ci_upper = (\n",
    "            np.mean(test_new_vid_old_subj_scores),\n",
    "            np.percentile(test_new_vid_old_subj_scores, 2.5),\n",
    "            np.percentile(test_new_vid_old_subj_scores, 97.5),\n",
    "        )\n",
    "\n",
    "        results_list.append({\n",
    "            \"model\": name,\n",
    "            \"test_mean\": test_mean,\n",
    "            \"test_ci_lower\": test_ci_lower,\n",
    "            \"test_ci_upper\": test_ci_upper,\n",
    "            \"test_new_subj_old_vid_mean\": test_new_subj_old_vid_mean,\n",
    "            \"test_new_subj_old_vid_ci_lower\": test_new_subj_old_vid_ci_lower,\n",
    "            \"test_new_subj_old_vid_ci_upper\": test_new_subj_old_vid_ci_upper,\n",
    "            \"test_new_vid_old_subj_mean\": test_new_vid_old_subj_mean,\n",
    "            \"test_new_vid_old_subj_ci_lower\": test_new_vid_old_subj_ci_lower,\n",
    "            \"test_new_vid_old_subj_ci_upper\": test_new_vid_old_subj_ci_upper\n",
    "        })\n",
    "\n",
    "        if verb:\n",
    "            print(f\"Fitted {name} with mean balanced accuracy: {test_mean:.3f}\")\n",
    "\n",
    "    return pd.DataFrame(results_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aa904fb8-4901-4a6d-852e-91a051efa488",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import balanced_accuracy_score, accuracy_score\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Создаем словарь моделей классификации\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(random_state=42),\n",
    "    \"Random Forest\": RandomForestClassifier(random_state=42),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(random_state=42),\n",
    "    \"Support Vector Machine\": SVC(),\n",
    "    \"K-Nearest Neighbors\": KNeighborsClassifier(),\n",
    "    \"Naive Bayes\": GaussianNB(),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
    "    \"XGBoost\": XGBClassifier(random_state=42,\n",
    "                            learning_rate=0.1,\n",
    "                            n_jobs=-1)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "436a090b-d27b-4d12-9d4d-e2a7a1aa87cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:2458: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted Logistic Regression with mean balanced accuracy: 0.186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:2458: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted Random Forest with mean balanced accuracy: 0.284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:2458: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted Gradient Boosting with mean balanced accuracy: 0.203\n",
      "Fitted Support Vector Machine with mean balanced accuracy: 0.238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:2458: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted K-Nearest Neighbors with mean balanced accuracy: 0.247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:2458: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted Naive Bayes with mean balanced accuracy: 0.337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:2458: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted Decision Tree with mean balanced accuracy: 0.317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:2458: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted XGBoost with mean balanced accuracy: 0.270\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>test_mean</th>\n",
       "      <th>test_ci_lower</th>\n",
       "      <th>test_ci_upper</th>\n",
       "      <th>test_new_subj_old_vid_mean</th>\n",
       "      <th>test_new_subj_old_vid_ci_lower</th>\n",
       "      <th>test_new_subj_old_vid_ci_upper</th>\n",
       "      <th>test_new_vid_old_subj_mean</th>\n",
       "      <th>test_new_vid_old_subj_ci_lower</th>\n",
       "      <th>test_new_vid_old_subj_ci_upper</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.185562</td>\n",
       "      <td>0.095956</td>\n",
       "      <td>0.284376</td>\n",
       "      <td>0.288696</td>\n",
       "      <td>0.217867</td>\n",
       "      <td>0.356978</td>\n",
       "      <td>0.254310</td>\n",
       "      <td>0.196226</td>\n",
       "      <td>0.313804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.283524</td>\n",
       "      <td>0.163038</td>\n",
       "      <td>0.412760</td>\n",
       "      <td>0.234588</td>\n",
       "      <td>0.184839</td>\n",
       "      <td>0.288498</td>\n",
       "      <td>0.239601</td>\n",
       "      <td>0.186381</td>\n",
       "      <td>0.296367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>0.202563</td>\n",
       "      <td>0.120802</td>\n",
       "      <td>0.292871</td>\n",
       "      <td>0.213975</td>\n",
       "      <td>0.163287</td>\n",
       "      <td>0.268828</td>\n",
       "      <td>0.240784</td>\n",
       "      <td>0.183451</td>\n",
       "      <td>0.301975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Support Vector Machine</td>\n",
       "      <td>0.237538</td>\n",
       "      <td>0.208333</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.253145</td>\n",
       "      <td>0.238462</td>\n",
       "      <td>0.271752</td>\n",
       "      <td>0.260072</td>\n",
       "      <td>0.240810</td>\n",
       "      <td>0.283584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>K-Nearest Neighbors</td>\n",
       "      <td>0.247343</td>\n",
       "      <td>0.131129</td>\n",
       "      <td>0.369048</td>\n",
       "      <td>0.220666</td>\n",
       "      <td>0.170254</td>\n",
       "      <td>0.279180</td>\n",
       "      <td>0.240318</td>\n",
       "      <td>0.181554</td>\n",
       "      <td>0.302864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.336594</td>\n",
       "      <td>0.204746</td>\n",
       "      <td>0.485814</td>\n",
       "      <td>0.241763</td>\n",
       "      <td>0.195005</td>\n",
       "      <td>0.289365</td>\n",
       "      <td>0.250749</td>\n",
       "      <td>0.190211</td>\n",
       "      <td>0.309006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.316755</td>\n",
       "      <td>0.175591</td>\n",
       "      <td>0.475069</td>\n",
       "      <td>0.280121</td>\n",
       "      <td>0.221747</td>\n",
       "      <td>0.346798</td>\n",
       "      <td>0.254533</td>\n",
       "      <td>0.192159</td>\n",
       "      <td>0.319317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.270177</td>\n",
       "      <td>0.144214</td>\n",
       "      <td>0.412932</td>\n",
       "      <td>0.247440</td>\n",
       "      <td>0.194054</td>\n",
       "      <td>0.304745</td>\n",
       "      <td>0.267395</td>\n",
       "      <td>0.204169</td>\n",
       "      <td>0.334573</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    model  test_mean  test_ci_lower  test_ci_upper  \\\n",
       "0     Logistic Regression   0.185562       0.095956       0.284376   \n",
       "1           Random Forest   0.283524       0.163038       0.412760   \n",
       "2       Gradient Boosting   0.202563       0.120802       0.292871   \n",
       "3  Support Vector Machine   0.237538       0.208333       0.250000   \n",
       "4     K-Nearest Neighbors   0.247343       0.131129       0.369048   \n",
       "5             Naive Bayes   0.336594       0.204746       0.485814   \n",
       "6           Decision Tree   0.316755       0.175591       0.475069   \n",
       "7                 XGBoost   0.270177       0.144214       0.412932   \n",
       "\n",
       "   test_new_subj_old_vid_mean  test_new_subj_old_vid_ci_lower  \\\n",
       "0                    0.288696                        0.217867   \n",
       "1                    0.234588                        0.184839   \n",
       "2                    0.213975                        0.163287   \n",
       "3                    0.253145                        0.238462   \n",
       "4                    0.220666                        0.170254   \n",
       "5                    0.241763                        0.195005   \n",
       "6                    0.280121                        0.221747   \n",
       "7                    0.247440                        0.194054   \n",
       "\n",
       "   test_new_subj_old_vid_ci_upper  test_new_vid_old_subj_mean  \\\n",
       "0                        0.356978                    0.254310   \n",
       "1                        0.288498                    0.239601   \n",
       "2                        0.268828                    0.240784   \n",
       "3                        0.271752                    0.260072   \n",
       "4                        0.279180                    0.240318   \n",
       "5                        0.289365                    0.250749   \n",
       "6                        0.346798                    0.254533   \n",
       "7                        0.304745                    0.267395   \n",
       "\n",
       "   test_new_vid_old_subj_ci_lower  test_new_vid_old_subj_ci_upper  \n",
       "0                        0.196226                        0.313804  \n",
       "1                        0.186381                        0.296367  \n",
       "2                        0.183451                        0.301975  \n",
       "3                        0.240810                        0.283584  \n",
       "4                        0.181554                        0.302864  \n",
       "5                        0.190211                        0.309006  \n",
       "6                        0.192159                        0.319317  \n",
       "7                        0.204169                        0.334573  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# результаты для var_features нормализованные без бейзлайна\n",
    "df_result = train_and_test_classifier(models, train_df_epochs, test_df_epochs,\n",
    "                              test_new_subj_old_vid_epochs, test_new_vid_old_subj_epochs)\n",
    "df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c50759c-552e-493d-8ea2-30210e682a30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
